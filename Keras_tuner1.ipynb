{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras_tuner1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMbrg71i1W3kxQx0KQVPCIm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JaeDoo1034/Kaggle-Study/blob/master/Keras_tuner1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns95RlRl24vi",
        "outputId": "ea77453c-4c2c-47cf-b956-91f1f6560b64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!pip install git+https://github.com/keras-team/keras-tuner.git -q"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAlJTJkR2-GS"
      },
      "source": [
        "MoA: Keras + KerasTuner best practicesÂ¶<br>\n",
        "This notebook will teach you how to:<br>\n",
        "\n",
        "1. Use a Keras neural network for the MoA competition\n",
        "2. Use KerasTuner to find high-performing model configurations\n",
        "3. Ensemble a few of the top models to generate final predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1SV7j1r27tn"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvUussiA3N_1",
        "outputId": "f021fed0-7c20-434e-cac1-b91cb1048de4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print('TF version:', tf.__version__)\n",
        "print('GPU devices:', tf.config.list_physical_devices('GPU'))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF version: 2.3.0\n",
            "GPU devices: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg1akox_3YbU"
      },
      "source": [
        "In this competition, we're looking at 3 CSV files: one for training features, one for training targets (with the same number of entries and a 1:1 match between entries in the features file and those in the targets file), and one for test features. The goal is to predict the targets that correspond to the test features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6f4zY_Y39wH",
        "outputId": "1cfd2fdc-7e48-4499-e8cd-59a578f39a82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aX9swJJ-3Tn_"
      },
      "source": [
        "train_features_df = pd.read_csv('/content/drive/My Drive/Data/train_features.csv')\n",
        "train_targets_df = pd.read_csv('/content/drive/My Drive/Data/train_targets_scored.csv')\n",
        "test_features_df = pd.read_csv('/content/drive/My Drive/Data/test_features.csv')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCpQV_Sl3uTw",
        "outputId": "62d0de6c-bcba-49a2-af9c-d200140c764d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print('train_features_df.shape:', train_features_df.shape)\n",
        "print('train_targets_df.shape:', train_targets_df.shape)\n",
        "print('test_features_df.shape:', test_features_df.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_features_df.shape: (23814, 876)\n",
            "train_targets_df.shape: (23814, 207)\n",
            "test_features_df.shape: (3982, 876)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooFE2hzA6LXB",
        "outputId": "f30a1afc-4b45-4ee4-89c9-25e75ba1d4f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "source": [
        "train_features_df.sample(5)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sig_id</th>\n",
              "      <th>cp_type</th>\n",
              "      <th>cp_time</th>\n",
              "      <th>cp_dose</th>\n",
              "      <th>g-0</th>\n",
              "      <th>g-1</th>\n",
              "      <th>g-2</th>\n",
              "      <th>g-3</th>\n",
              "      <th>g-4</th>\n",
              "      <th>g-5</th>\n",
              "      <th>g-6</th>\n",
              "      <th>g-7</th>\n",
              "      <th>g-8</th>\n",
              "      <th>g-9</th>\n",
              "      <th>g-10</th>\n",
              "      <th>g-11</th>\n",
              "      <th>g-12</th>\n",
              "      <th>g-13</th>\n",
              "      <th>g-14</th>\n",
              "      <th>g-15</th>\n",
              "      <th>g-16</th>\n",
              "      <th>g-17</th>\n",
              "      <th>g-18</th>\n",
              "      <th>g-19</th>\n",
              "      <th>g-20</th>\n",
              "      <th>g-21</th>\n",
              "      <th>g-22</th>\n",
              "      <th>g-23</th>\n",
              "      <th>g-24</th>\n",
              "      <th>g-25</th>\n",
              "      <th>g-26</th>\n",
              "      <th>g-27</th>\n",
              "      <th>g-28</th>\n",
              "      <th>g-29</th>\n",
              "      <th>g-30</th>\n",
              "      <th>g-31</th>\n",
              "      <th>g-32</th>\n",
              "      <th>g-33</th>\n",
              "      <th>g-34</th>\n",
              "      <th>g-35</th>\n",
              "      <th>...</th>\n",
              "      <th>c-60</th>\n",
              "      <th>c-61</th>\n",
              "      <th>c-62</th>\n",
              "      <th>c-63</th>\n",
              "      <th>c-64</th>\n",
              "      <th>c-65</th>\n",
              "      <th>c-66</th>\n",
              "      <th>c-67</th>\n",
              "      <th>c-68</th>\n",
              "      <th>c-69</th>\n",
              "      <th>c-70</th>\n",
              "      <th>c-71</th>\n",
              "      <th>c-72</th>\n",
              "      <th>c-73</th>\n",
              "      <th>c-74</th>\n",
              "      <th>c-75</th>\n",
              "      <th>c-76</th>\n",
              "      <th>c-77</th>\n",
              "      <th>c-78</th>\n",
              "      <th>c-79</th>\n",
              "      <th>c-80</th>\n",
              "      <th>c-81</th>\n",
              "      <th>c-82</th>\n",
              "      <th>c-83</th>\n",
              "      <th>c-84</th>\n",
              "      <th>c-85</th>\n",
              "      <th>c-86</th>\n",
              "      <th>c-87</th>\n",
              "      <th>c-88</th>\n",
              "      <th>c-89</th>\n",
              "      <th>c-90</th>\n",
              "      <th>c-91</th>\n",
              "      <th>c-92</th>\n",
              "      <th>c-93</th>\n",
              "      <th>c-94</th>\n",
              "      <th>c-95</th>\n",
              "      <th>c-96</th>\n",
              "      <th>c-97</th>\n",
              "      <th>c-98</th>\n",
              "      <th>c-99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>778</th>\n",
              "      <td>id_07ecb3e85</td>\n",
              "      <td>trt_cp</td>\n",
              "      <td>24</td>\n",
              "      <td>D1</td>\n",
              "      <td>0.3388</td>\n",
              "      <td>-0.9912</td>\n",
              "      <td>0.2248</td>\n",
              "      <td>0.3389</td>\n",
              "      <td>1.9690</td>\n",
              "      <td>-0.4671</td>\n",
              "      <td>-0.1002</td>\n",
              "      <td>0.4639</td>\n",
              "      <td>-0.0698</td>\n",
              "      <td>-0.2725</td>\n",
              "      <td>-0.0814</td>\n",
              "      <td>0.5440</td>\n",
              "      <td>-0.5042</td>\n",
              "      <td>-0.0908</td>\n",
              "      <td>0.1309</td>\n",
              "      <td>-0.6146</td>\n",
              "      <td>1.5780</td>\n",
              "      <td>0.1536</td>\n",
              "      <td>0.2466</td>\n",
              "      <td>-1.6070</td>\n",
              "      <td>-0.5536</td>\n",
              "      <td>-0.5646</td>\n",
              "      <td>0.2880</td>\n",
              "      <td>-0.0313</td>\n",
              "      <td>-0.2242</td>\n",
              "      <td>-0.3258</td>\n",
              "      <td>-0.5939</td>\n",
              "      <td>-0.3500</td>\n",
              "      <td>-0.3767</td>\n",
              "      <td>-0.1860</td>\n",
              "      <td>-0.2111</td>\n",
              "      <td>-0.0155</td>\n",
              "      <td>-1.3370</td>\n",
              "      <td>0.0639</td>\n",
              "      <td>0.0883</td>\n",
              "      <td>-0.3990</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.6768</td>\n",
              "      <td>-0.7074</td>\n",
              "      <td>-0.7677</td>\n",
              "      <td>0.1235</td>\n",
              "      <td>-1.2360</td>\n",
              "      <td>0.0547</td>\n",
              "      <td>-0.1787</td>\n",
              "      <td>-0.0042</td>\n",
              "      <td>-0.1828</td>\n",
              "      <td>-0.2835</td>\n",
              "      <td>0.4365</td>\n",
              "      <td>-0.4902</td>\n",
              "      <td>-0.2675</td>\n",
              "      <td>-0.2333</td>\n",
              "      <td>-0.3714</td>\n",
              "      <td>-0.1162</td>\n",
              "      <td>-0.4374</td>\n",
              "      <td>-0.7873</td>\n",
              "      <td>0.6983</td>\n",
              "      <td>-0.6884</td>\n",
              "      <td>-0.2678</td>\n",
              "      <td>-1.2700</td>\n",
              "      <td>-0.3111</td>\n",
              "      <td>-0.6138</td>\n",
              "      <td>-0.3663</td>\n",
              "      <td>0.1876</td>\n",
              "      <td>-0.1194</td>\n",
              "      <td>-0.4981</td>\n",
              "      <td>-0.0688</td>\n",
              "      <td>-0.2777</td>\n",
              "      <td>-0.5158</td>\n",
              "      <td>0.4552</td>\n",
              "      <td>-0.3342</td>\n",
              "      <td>-1.8190</td>\n",
              "      <td>-1.0680</td>\n",
              "      <td>-0.7455</td>\n",
              "      <td>0.0101</td>\n",
              "      <td>-0.5123</td>\n",
              "      <td>-0.3272</td>\n",
              "      <td>-0.5126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4641</th>\n",
              "      <td>id_31edc89ef</td>\n",
              "      <td>trt_cp</td>\n",
              "      <td>24</td>\n",
              "      <td>D1</td>\n",
              "      <td>-0.3372</td>\n",
              "      <td>-0.1891</td>\n",
              "      <td>2.5860</td>\n",
              "      <td>-0.6923</td>\n",
              "      <td>-0.9179</td>\n",
              "      <td>1.5910</td>\n",
              "      <td>-0.0738</td>\n",
              "      <td>0.2384</td>\n",
              "      <td>0.6933</td>\n",
              "      <td>1.6440</td>\n",
              "      <td>1.1620</td>\n",
              "      <td>-1.8300</td>\n",
              "      <td>-0.5271</td>\n",
              "      <td>-0.1228</td>\n",
              "      <td>-0.1120</td>\n",
              "      <td>-0.0770</td>\n",
              "      <td>0.2116</td>\n",
              "      <td>-0.3374</td>\n",
              "      <td>0.1751</td>\n",
              "      <td>-0.2421</td>\n",
              "      <td>1.4090</td>\n",
              "      <td>0.1292</td>\n",
              "      <td>-0.0394</td>\n",
              "      <td>-0.0340</td>\n",
              "      <td>-0.1540</td>\n",
              "      <td>-0.3701</td>\n",
              "      <td>-0.1268</td>\n",
              "      <td>0.1934</td>\n",
              "      <td>-0.3309</td>\n",
              "      <td>-0.3204</td>\n",
              "      <td>-0.5574</td>\n",
              "      <td>0.5905</td>\n",
              "      <td>-0.3395</td>\n",
              "      <td>-0.4635</td>\n",
              "      <td>0.0658</td>\n",
              "      <td>-1.0260</td>\n",
              "      <td>...</td>\n",
              "      <td>0.1636</td>\n",
              "      <td>0.6459</td>\n",
              "      <td>-0.0181</td>\n",
              "      <td>-0.6820</td>\n",
              "      <td>-0.4035</td>\n",
              "      <td>0.7838</td>\n",
              "      <td>0.1066</td>\n",
              "      <td>0.5167</td>\n",
              "      <td>0.0856</td>\n",
              "      <td>1.3730</td>\n",
              "      <td>-0.3473</td>\n",
              "      <td>-0.1488</td>\n",
              "      <td>0.6359</td>\n",
              "      <td>0.4595</td>\n",
              "      <td>0.5128</td>\n",
              "      <td>-0.3642</td>\n",
              "      <td>-0.0334</td>\n",
              "      <td>0.5751</td>\n",
              "      <td>0.7964</td>\n",
              "      <td>0.0514</td>\n",
              "      <td>0.2861</td>\n",
              "      <td>-0.1883</td>\n",
              "      <td>0.5591</td>\n",
              "      <td>-0.1685</td>\n",
              "      <td>-0.2859</td>\n",
              "      <td>-1.4610</td>\n",
              "      <td>0.9457</td>\n",
              "      <td>0.0362</td>\n",
              "      <td>0.1456</td>\n",
              "      <td>1.1760</td>\n",
              "      <td>-0.1916</td>\n",
              "      <td>-0.5433</td>\n",
              "      <td>-0.0800</td>\n",
              "      <td>0.3584</td>\n",
              "      <td>0.1172</td>\n",
              "      <td>0.1852</td>\n",
              "      <td>0.0601</td>\n",
              "      <td>0.5861</td>\n",
              "      <td>0.0626</td>\n",
              "      <td>-0.5796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3428</th>\n",
              "      <td>id_24cdaa657</td>\n",
              "      <td>trt_cp</td>\n",
              "      <td>48</td>\n",
              "      <td>D2</td>\n",
              "      <td>1.8700</td>\n",
              "      <td>-0.0563</td>\n",
              "      <td>-0.9304</td>\n",
              "      <td>-0.9547</td>\n",
              "      <td>-0.8074</td>\n",
              "      <td>-0.7664</td>\n",
              "      <td>-0.3451</td>\n",
              "      <td>-0.5916</td>\n",
              "      <td>-0.7421</td>\n",
              "      <td>-0.0138</td>\n",
              "      <td>0.5622</td>\n",
              "      <td>1.7050</td>\n",
              "      <td>-0.5498</td>\n",
              "      <td>-0.5419</td>\n",
              "      <td>0.5361</td>\n",
              "      <td>1.0730</td>\n",
              "      <td>1.0080</td>\n",
              "      <td>0.3818</td>\n",
              "      <td>0.1558</td>\n",
              "      <td>-0.5690</td>\n",
              "      <td>-0.8460</td>\n",
              "      <td>-0.5068</td>\n",
              "      <td>3.0210</td>\n",
              "      <td>-0.0045</td>\n",
              "      <td>1.2110</td>\n",
              "      <td>1.3170</td>\n",
              "      <td>0.4085</td>\n",
              "      <td>0.3413</td>\n",
              "      <td>-0.2189</td>\n",
              "      <td>0.8226</td>\n",
              "      <td>2.2110</td>\n",
              "      <td>-0.4719</td>\n",
              "      <td>-0.6977</td>\n",
              "      <td>3.5000</td>\n",
              "      <td>-1.3990</td>\n",
              "      <td>1.5130</td>\n",
              "      <td>...</td>\n",
              "      <td>0.4574</td>\n",
              "      <td>1.2700</td>\n",
              "      <td>0.1997</td>\n",
              "      <td>0.5201</td>\n",
              "      <td>-0.3351</td>\n",
              "      <td>0.7389</td>\n",
              "      <td>-0.9999</td>\n",
              "      <td>0.3063</td>\n",
              "      <td>0.8620</td>\n",
              "      <td>1.1900</td>\n",
              "      <td>0.9253</td>\n",
              "      <td>-0.9719</td>\n",
              "      <td>0.4908</td>\n",
              "      <td>-0.4926</td>\n",
              "      <td>0.5025</td>\n",
              "      <td>0.9047</td>\n",
              "      <td>0.4979</td>\n",
              "      <td>0.4417</td>\n",
              "      <td>-0.4749</td>\n",
              "      <td>0.6376</td>\n",
              "      <td>0.4999</td>\n",
              "      <td>1.4200</td>\n",
              "      <td>-1.4430</td>\n",
              "      <td>-0.9039</td>\n",
              "      <td>0.6668</td>\n",
              "      <td>0.7409</td>\n",
              "      <td>0.9675</td>\n",
              "      <td>1.0550</td>\n",
              "      <td>1.3960</td>\n",
              "      <td>0.1777</td>\n",
              "      <td>0.8893</td>\n",
              "      <td>0.9004</td>\n",
              "      <td>-0.3938</td>\n",
              "      <td>0.2917</td>\n",
              "      <td>1.0420</td>\n",
              "      <td>-0.7796</td>\n",
              "      <td>-1.1200</td>\n",
              "      <td>1.2320</td>\n",
              "      <td>-0.3820</td>\n",
              "      <td>-0.3763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5637</th>\n",
              "      <td>id_3caa1f427</td>\n",
              "      <td>trt_cp</td>\n",
              "      <td>48</td>\n",
              "      <td>D1</td>\n",
              "      <td>-0.2662</td>\n",
              "      <td>0.2736</td>\n",
              "      <td>1.3300</td>\n",
              "      <td>0.2343</td>\n",
              "      <td>-0.0490</td>\n",
              "      <td>-0.6133</td>\n",
              "      <td>0.5214</td>\n",
              "      <td>-0.4810</td>\n",
              "      <td>0.0571</td>\n",
              "      <td>-0.1016</td>\n",
              "      <td>-0.1306</td>\n",
              "      <td>1.1060</td>\n",
              "      <td>-1.0330</td>\n",
              "      <td>0.1677</td>\n",
              "      <td>-0.3200</td>\n",
              "      <td>0.4185</td>\n",
              "      <td>0.9928</td>\n",
              "      <td>-0.1261</td>\n",
              "      <td>-0.0556</td>\n",
              "      <td>0.3991</td>\n",
              "      <td>-0.1965</td>\n",
              "      <td>0.5626</td>\n",
              "      <td>0.9987</td>\n",
              "      <td>-0.3865</td>\n",
              "      <td>-1.0560</td>\n",
              "      <td>-0.6101</td>\n",
              "      <td>-0.8921</td>\n",
              "      <td>-0.5305</td>\n",
              "      <td>0.0367</td>\n",
              "      <td>0.6474</td>\n",
              "      <td>0.8108</td>\n",
              "      <td>0.1015</td>\n",
              "      <td>0.1932</td>\n",
              "      <td>0.6029</td>\n",
              "      <td>0.4967</td>\n",
              "      <td>-0.0910</td>\n",
              "      <td>...</td>\n",
              "      <td>0.5278</td>\n",
              "      <td>-0.6174</td>\n",
              "      <td>0.0678</td>\n",
              "      <td>0.4054</td>\n",
              "      <td>0.5273</td>\n",
              "      <td>0.6554</td>\n",
              "      <td>-0.2335</td>\n",
              "      <td>-0.2161</td>\n",
              "      <td>-0.4208</td>\n",
              "      <td>0.6477</td>\n",
              "      <td>0.3885</td>\n",
              "      <td>0.5999</td>\n",
              "      <td>1.0260</td>\n",
              "      <td>0.6834</td>\n",
              "      <td>0.3242</td>\n",
              "      <td>0.8176</td>\n",
              "      <td>1.7230</td>\n",
              "      <td>-0.0353</td>\n",
              "      <td>0.3669</td>\n",
              "      <td>-0.5014</td>\n",
              "      <td>0.4252</td>\n",
              "      <td>0.7724</td>\n",
              "      <td>0.1996</td>\n",
              "      <td>-0.1291</td>\n",
              "      <td>0.6842</td>\n",
              "      <td>1.2680</td>\n",
              "      <td>0.7236</td>\n",
              "      <td>-0.5492</td>\n",
              "      <td>0.5560</td>\n",
              "      <td>0.1482</td>\n",
              "      <td>-0.2812</td>\n",
              "      <td>0.8796</td>\n",
              "      <td>-0.2169</td>\n",
              "      <td>1.3210</td>\n",
              "      <td>0.3599</td>\n",
              "      <td>-0.4869</td>\n",
              "      <td>0.1209</td>\n",
              "      <td>1.3480</td>\n",
              "      <td>0.4739</td>\n",
              "      <td>0.3978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>id_017e29d4d</td>\n",
              "      <td>trt_cp</td>\n",
              "      <td>24</td>\n",
              "      <td>D1</td>\n",
              "      <td>-0.7307</td>\n",
              "      <td>-0.6776</td>\n",
              "      <td>0.6388</td>\n",
              "      <td>0.0720</td>\n",
              "      <td>-0.1736</td>\n",
              "      <td>0.4976</td>\n",
              "      <td>1.0460</td>\n",
              "      <td>0.1471</td>\n",
              "      <td>0.2634</td>\n",
              "      <td>0.1624</td>\n",
              "      <td>0.1681</td>\n",
              "      <td>-0.6652</td>\n",
              "      <td>2.1350</td>\n",
              "      <td>0.2751</td>\n",
              "      <td>-0.4590</td>\n",
              "      <td>0.5600</td>\n",
              "      <td>2.4620</td>\n",
              "      <td>0.4094</td>\n",
              "      <td>-0.0337</td>\n",
              "      <td>0.3257</td>\n",
              "      <td>0.2078</td>\n",
              "      <td>0.7657</td>\n",
              "      <td>-0.2393</td>\n",
              "      <td>0.0893</td>\n",
              "      <td>0.4417</td>\n",
              "      <td>0.4058</td>\n",
              "      <td>0.2082</td>\n",
              "      <td>0.8765</td>\n",
              "      <td>0.1916</td>\n",
              "      <td>-0.4320</td>\n",
              "      <td>0.8011</td>\n",
              "      <td>4.3350</td>\n",
              "      <td>1.1000</td>\n",
              "      <td>0.5923</td>\n",
              "      <td>0.3206</td>\n",
              "      <td>-0.6777</td>\n",
              "      <td>...</td>\n",
              "      <td>0.5443</td>\n",
              "      <td>0.8304</td>\n",
              "      <td>0.0765</td>\n",
              "      <td>0.2288</td>\n",
              "      <td>0.1586</td>\n",
              "      <td>0.1783</td>\n",
              "      <td>0.2578</td>\n",
              "      <td>-0.3513</td>\n",
              "      <td>0.5269</td>\n",
              "      <td>-0.8990</td>\n",
              "      <td>-0.0010</td>\n",
              "      <td>0.8235</td>\n",
              "      <td>-0.2662</td>\n",
              "      <td>-0.0614</td>\n",
              "      <td>0.6772</td>\n",
              "      <td>0.4411</td>\n",
              "      <td>0.5389</td>\n",
              "      <td>-0.4541</td>\n",
              "      <td>0.0095</td>\n",
              "      <td>0.2137</td>\n",
              "      <td>0.1142</td>\n",
              "      <td>0.3014</td>\n",
              "      <td>0.1031</td>\n",
              "      <td>0.4253</td>\n",
              "      <td>0.4498</td>\n",
              "      <td>0.2493</td>\n",
              "      <td>-0.8700</td>\n",
              "      <td>0.4126</td>\n",
              "      <td>-0.1882</td>\n",
              "      <td>0.0735</td>\n",
              "      <td>-1.3320</td>\n",
              "      <td>0.2909</td>\n",
              "      <td>1.1330</td>\n",
              "      <td>-0.7993</td>\n",
              "      <td>0.5675</td>\n",
              "      <td>0.5350</td>\n",
              "      <td>0.2375</td>\n",
              "      <td>0.6432</td>\n",
              "      <td>-0.6192</td>\n",
              "      <td>0.6445</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã 876 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            sig_id cp_type  cp_time cp_dose  ...    c-96    c-97    c-98    c-99\n",
              "778   id_07ecb3e85  trt_cp       24      D1  ...  0.0101 -0.5123 -0.3272 -0.5126\n",
              "4641  id_31edc89ef  trt_cp       24      D1  ...  0.0601  0.5861  0.0626 -0.5796\n",
              "3428  id_24cdaa657  trt_cp       48      D2  ... -1.1200  1.2320 -0.3820 -0.3763\n",
              "5637  id_3caa1f427  trt_cp       48      D1  ...  0.1209  1.3480  0.4739  0.3978\n",
              "131   id_017e29d4d  trt_cp       24      D1  ...  0.2375  0.6432 -0.6192  0.6445\n",
              "\n",
              "[5 rows x 876 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xnj7El6A6SOW"
      },
      "source": [
        "Ok, so we have 2 categorical features (cp_type and cp_dose, which are strings), and everything else is numerical (assuming g-0 to g-99 are homogeneous in type).\n",
        "\n",
        "We'll use the StringLookup and CategoryEncoding layers to encode the categorical features, and the Normalization layer to normalize the values of the numerical features.\n",
        "\n",
        "Let's look at the targets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTMgjMEi6NvN",
        "outputId": "6c98c4bb-8915-4986-a858-78ba39a6cb5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "source": [
        "train_targets_df.sample(5)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sig_id</th>\n",
              "      <th>5-alpha_reductase_inhibitor</th>\n",
              "      <th>11-beta-hsd1_inhibitor</th>\n",
              "      <th>acat_inhibitor</th>\n",
              "      <th>acetylcholine_receptor_agonist</th>\n",
              "      <th>acetylcholine_receptor_antagonist</th>\n",
              "      <th>acetylcholinesterase_inhibitor</th>\n",
              "      <th>adenosine_receptor_agonist</th>\n",
              "      <th>adenosine_receptor_antagonist</th>\n",
              "      <th>adenylyl_cyclase_activator</th>\n",
              "      <th>adrenergic_receptor_agonist</th>\n",
              "      <th>adrenergic_receptor_antagonist</th>\n",
              "      <th>akt_inhibitor</th>\n",
              "      <th>aldehyde_dehydrogenase_inhibitor</th>\n",
              "      <th>alk_inhibitor</th>\n",
              "      <th>ampk_activator</th>\n",
              "      <th>analgesic</th>\n",
              "      <th>androgen_receptor_agonist</th>\n",
              "      <th>androgen_receptor_antagonist</th>\n",
              "      <th>anesthetic_-_local</th>\n",
              "      <th>angiogenesis_inhibitor</th>\n",
              "      <th>angiotensin_receptor_antagonist</th>\n",
              "      <th>anti-inflammatory</th>\n",
              "      <th>antiarrhythmic</th>\n",
              "      <th>antibiotic</th>\n",
              "      <th>anticonvulsant</th>\n",
              "      <th>antifungal</th>\n",
              "      <th>antihistamine</th>\n",
              "      <th>antimalarial</th>\n",
              "      <th>antioxidant</th>\n",
              "      <th>antiprotozoal</th>\n",
              "      <th>antiviral</th>\n",
              "      <th>apoptosis_stimulant</th>\n",
              "      <th>aromatase_inhibitor</th>\n",
              "      <th>atm_kinase_inhibitor</th>\n",
              "      <th>atp-sensitive_potassium_channel_antagonist</th>\n",
              "      <th>atp_synthase_inhibitor</th>\n",
              "      <th>atpase_inhibitor</th>\n",
              "      <th>atr_kinase_inhibitor</th>\n",
              "      <th>aurora_kinase_inhibitor</th>\n",
              "      <th>...</th>\n",
              "      <th>protein_synthesis_inhibitor</th>\n",
              "      <th>protein_tyrosine_kinase_inhibitor</th>\n",
              "      <th>radiopaque_medium</th>\n",
              "      <th>raf_inhibitor</th>\n",
              "      <th>ras_gtpase_inhibitor</th>\n",
              "      <th>retinoid_receptor_agonist</th>\n",
              "      <th>retinoid_receptor_antagonist</th>\n",
              "      <th>rho_associated_kinase_inhibitor</th>\n",
              "      <th>ribonucleoside_reductase_inhibitor</th>\n",
              "      <th>rna_polymerase_inhibitor</th>\n",
              "      <th>serotonin_receptor_agonist</th>\n",
              "      <th>serotonin_receptor_antagonist</th>\n",
              "      <th>serotonin_reuptake_inhibitor</th>\n",
              "      <th>sigma_receptor_agonist</th>\n",
              "      <th>sigma_receptor_antagonist</th>\n",
              "      <th>smoothened_receptor_antagonist</th>\n",
              "      <th>sodium_channel_inhibitor</th>\n",
              "      <th>sphingosine_receptor_agonist</th>\n",
              "      <th>src_inhibitor</th>\n",
              "      <th>steroid</th>\n",
              "      <th>syk_inhibitor</th>\n",
              "      <th>tachykinin_antagonist</th>\n",
              "      <th>tgf-beta_receptor_inhibitor</th>\n",
              "      <th>thrombin_inhibitor</th>\n",
              "      <th>thymidylate_synthase_inhibitor</th>\n",
              "      <th>tlr_agonist</th>\n",
              "      <th>tlr_antagonist</th>\n",
              "      <th>tnf_inhibitor</th>\n",
              "      <th>topoisomerase_inhibitor</th>\n",
              "      <th>transient_receptor_potential_channel_antagonist</th>\n",
              "      <th>tropomyosin_receptor_kinase_inhibitor</th>\n",
              "      <th>trpv_agonist</th>\n",
              "      <th>trpv_antagonist</th>\n",
              "      <th>tubulin_inhibitor</th>\n",
              "      <th>tyrosine_kinase_inhibitor</th>\n",
              "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
              "      <th>vegfr_inhibitor</th>\n",
              "      <th>vitamin_b</th>\n",
              "      <th>vitamin_d_receptor_agonist</th>\n",
              "      <th>wnt_inhibitor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>16143</th>\n",
              "      <td>id_ad9b9a725</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14888</th>\n",
              "      <td>id_a012b7f60</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16289</th>\n",
              "      <td>id_af45a4c71</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3974</th>\n",
              "      <td>id_2a7c08eb6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22150</th>\n",
              "      <td>id_ed9355bf5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã 207 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             sig_id  ...  wnt_inhibitor\n",
              "16143  id_ad9b9a725  ...              0\n",
              "14888  id_a012b7f60  ...              0\n",
              "16289  id_af45a4c71  ...              0\n",
              "3974   id_2a7c08eb6  ...              0\n",
              "22150  id_ed9355bf5  ...              0\n",
              "\n",
              "[5 rows x 207 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3mIbFMl6Yv7"
      },
      "source": [
        "The targets are binary indicators (0 or 1) across 206 different categories. So our model should output a probability score between 0 and 1 (sigmoid activation) across 206 outputs.\n",
        "\n",
        "The sample submission format matches these expectations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oylS9Sjy6VB9",
        "outputId": "507df1a8-4d31-4c12-bdd1-683ed5aaf258",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "source": [
        "sample_submission_df = pd.read_csv('/content/drive/My Drive/Data/sample_submission.csv')\n",
        "sample_submission_df.sample(5)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sig_id</th>\n",
              "      <th>5-alpha_reductase_inhibitor</th>\n",
              "      <th>11-beta-hsd1_inhibitor</th>\n",
              "      <th>acat_inhibitor</th>\n",
              "      <th>acetylcholine_receptor_agonist</th>\n",
              "      <th>acetylcholine_receptor_antagonist</th>\n",
              "      <th>acetylcholinesterase_inhibitor</th>\n",
              "      <th>adenosine_receptor_agonist</th>\n",
              "      <th>adenosine_receptor_antagonist</th>\n",
              "      <th>adenylyl_cyclase_activator</th>\n",
              "      <th>adrenergic_receptor_agonist</th>\n",
              "      <th>adrenergic_receptor_antagonist</th>\n",
              "      <th>akt_inhibitor</th>\n",
              "      <th>aldehyde_dehydrogenase_inhibitor</th>\n",
              "      <th>alk_inhibitor</th>\n",
              "      <th>ampk_activator</th>\n",
              "      <th>analgesic</th>\n",
              "      <th>androgen_receptor_agonist</th>\n",
              "      <th>androgen_receptor_antagonist</th>\n",
              "      <th>anesthetic_-_local</th>\n",
              "      <th>angiogenesis_inhibitor</th>\n",
              "      <th>angiotensin_receptor_antagonist</th>\n",
              "      <th>anti-inflammatory</th>\n",
              "      <th>antiarrhythmic</th>\n",
              "      <th>antibiotic</th>\n",
              "      <th>anticonvulsant</th>\n",
              "      <th>antifungal</th>\n",
              "      <th>antihistamine</th>\n",
              "      <th>antimalarial</th>\n",
              "      <th>antioxidant</th>\n",
              "      <th>antiprotozoal</th>\n",
              "      <th>antiviral</th>\n",
              "      <th>apoptosis_stimulant</th>\n",
              "      <th>aromatase_inhibitor</th>\n",
              "      <th>atm_kinase_inhibitor</th>\n",
              "      <th>atp-sensitive_potassium_channel_antagonist</th>\n",
              "      <th>atp_synthase_inhibitor</th>\n",
              "      <th>atpase_inhibitor</th>\n",
              "      <th>atr_kinase_inhibitor</th>\n",
              "      <th>aurora_kinase_inhibitor</th>\n",
              "      <th>...</th>\n",
              "      <th>protein_synthesis_inhibitor</th>\n",
              "      <th>protein_tyrosine_kinase_inhibitor</th>\n",
              "      <th>radiopaque_medium</th>\n",
              "      <th>raf_inhibitor</th>\n",
              "      <th>ras_gtpase_inhibitor</th>\n",
              "      <th>retinoid_receptor_agonist</th>\n",
              "      <th>retinoid_receptor_antagonist</th>\n",
              "      <th>rho_associated_kinase_inhibitor</th>\n",
              "      <th>ribonucleoside_reductase_inhibitor</th>\n",
              "      <th>rna_polymerase_inhibitor</th>\n",
              "      <th>serotonin_receptor_agonist</th>\n",
              "      <th>serotonin_receptor_antagonist</th>\n",
              "      <th>serotonin_reuptake_inhibitor</th>\n",
              "      <th>sigma_receptor_agonist</th>\n",
              "      <th>sigma_receptor_antagonist</th>\n",
              "      <th>smoothened_receptor_antagonist</th>\n",
              "      <th>sodium_channel_inhibitor</th>\n",
              "      <th>sphingosine_receptor_agonist</th>\n",
              "      <th>src_inhibitor</th>\n",
              "      <th>steroid</th>\n",
              "      <th>syk_inhibitor</th>\n",
              "      <th>tachykinin_antagonist</th>\n",
              "      <th>tgf-beta_receptor_inhibitor</th>\n",
              "      <th>thrombin_inhibitor</th>\n",
              "      <th>thymidylate_synthase_inhibitor</th>\n",
              "      <th>tlr_agonist</th>\n",
              "      <th>tlr_antagonist</th>\n",
              "      <th>tnf_inhibitor</th>\n",
              "      <th>topoisomerase_inhibitor</th>\n",
              "      <th>transient_receptor_potential_channel_antagonist</th>\n",
              "      <th>tropomyosin_receptor_kinase_inhibitor</th>\n",
              "      <th>trpv_agonist</th>\n",
              "      <th>trpv_antagonist</th>\n",
              "      <th>tubulin_inhibitor</th>\n",
              "      <th>tyrosine_kinase_inhibitor</th>\n",
              "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
              "      <th>vegfr_inhibitor</th>\n",
              "      <th>vitamin_b</th>\n",
              "      <th>vitamin_d_receptor_agonist</th>\n",
              "      <th>wnt_inhibitor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3070</th>\n",
              "      <td>id_c6d9b85a5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1678</th>\n",
              "      <td>id_6cdd13153</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1542</th>\n",
              "      <td>id_6388f978f</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1297</th>\n",
              "      <td>id_53fd1f636</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2710</th>\n",
              "      <td>id_ae1206b60</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã 207 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            sig_id  ...  wnt_inhibitor\n",
              "3070  id_c6d9b85a5  ...            0.5\n",
              "1678  id_6cdd13153  ...            0.5\n",
              "1542  id_6388f978f  ...            0.5\n",
              "1297  id_53fd1f636  ...            0.5\n",
              "2710  id_ae1206b60  ...            0.5\n",
              "\n",
              "[5 rows x 207 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5mzXVzO6se9"
      },
      "source": [
        "Out of 23,814 samples, how often is each of the 206 target indicators positive?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Cq92Vun6piX",
        "outputId": "29638bae-5e3b-4d65-a0e5-7933ddfa1ff0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for target_name in list(train_targets_df)[1:]:\n",
        "  rate = float(sum(train_targets_df[target_name])) / len(train_targets_df)\n",
        "  print('%.4f percent positivity rate for %s' % (100*rate, target_name) )"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0714 percent positivity rate for 5-alpha_reductase_inhibitor\n",
            "0.0756 percent positivity rate for 11-beta-hsd1_inhibitor\n",
            "0.1008 percent positivity rate for acat_inhibitor\n",
            "0.7979 percent positivity rate for acetylcholine_receptor_agonist\n",
            "1.2640 percent positivity rate for acetylcholine_receptor_antagonist\n",
            "0.3065 percent positivity rate for acetylcholinesterase_inhibitor\n",
            "0.2268 percent positivity rate for adenosine_receptor_agonist\n",
            "0.4031 percent positivity rate for adenosine_receptor_antagonist\n",
            "0.0504 percent positivity rate for adenylyl_cyclase_activator\n",
            "1.1338 percent positivity rate for adrenergic_receptor_agonist\n",
            "1.5117 percent positivity rate for adrenergic_receptor_antagonist\n",
            "0.2771 percent positivity rate for akt_inhibitor\n",
            "0.0294 percent positivity rate for aldehyde_dehydrogenase_inhibitor\n",
            "0.1764 percent positivity rate for alk_inhibitor\n",
            "0.0504 percent positivity rate for ampk_activator\n",
            "0.0504 percent positivity rate for analgesic\n",
            "0.2016 percent positivity rate for androgen_receptor_agonist\n",
            "0.3737 percent positivity rate for androgen_receptor_antagonist\n",
            "0.3359 percent positivity rate for anesthetic_-_local\n",
            "0.1512 percent positivity rate for angiogenesis_inhibitor\n",
            "0.1554 percent positivity rate for angiotensin_receptor_antagonist\n",
            "0.3065 percent positivity rate for anti-inflammatory\n",
            "0.0252 percent positivity rate for antiarrhythmic\n",
            "0.1806 percent positivity rate for antibiotic\n",
            "0.0504 percent positivity rate for anticonvulsant\n",
            "0.0546 percent positivity rate for antifungal\n",
            "0.0504 percent positivity rate for antihistamine\n",
            "0.0756 percent positivity rate for antimalarial\n",
            "0.3065 percent positivity rate for antioxidant\n",
            "0.1512 percent positivity rate for antiprotozoal\n",
            "0.0966 percent positivity rate for antiviral\n",
            "0.2058 percent positivity rate for apoptosis_stimulant\n",
            "0.1974 percent positivity rate for aromatase_inhibitor\n",
            "0.0252 percent positivity rate for atm_kinase_inhibitor\n",
            "0.0042 percent positivity rate for atp-sensitive_potassium_channel_antagonist\n",
            "0.0504 percent positivity rate for atp_synthase_inhibitor\n",
            "0.4073 percent positivity rate for atpase_inhibitor\n",
            "0.0798 percent positivity rate for atr_kinase_inhibitor\n",
            "0.4031 percent positivity rate for aurora_kinase_inhibitor\n",
            "0.0252 percent positivity rate for autotaxin_inhibitor\n",
            "0.2520 percent positivity rate for bacterial_30s_ribosomal_subunit_inhibitor\n",
            "0.3359 percent positivity rate for bacterial_50s_ribosomal_subunit_inhibitor\n",
            "0.1512 percent positivity rate for bacterial_antifolate\n",
            "0.8062 percent positivity rate for bacterial_cell_wall_synthesis_inhibitor\n",
            "0.3737 percent positivity rate for bacterial_dna_gyrase_inhibitor\n",
            "0.4829 percent positivity rate for bacterial_dna_inhibitor\n",
            "0.0294 percent positivity rate for bacterial_membrane_integrity_inhibitor\n",
            "0.1302 percent positivity rate for bcl_inhibitor\n",
            "0.1596 percent positivity rate for bcr-abl_inhibitor\n",
            "0.2813 percent positivity rate for benzodiazepine_receptor_agonist\n",
            "0.1008 percent positivity rate for beta_amyloid_inhibitor\n",
            "0.2855 percent positivity rate for bromodomain_inhibitor\n",
            "0.1218 percent positivity rate for btk_inhibitor\n",
            "0.0252 percent positivity rate for calcineurin_inhibitor\n",
            "1.1800 percent positivity rate for calcium_channel_blocker\n",
            "0.1764 percent positivity rate for cannabinoid_receptor_agonist\n",
            "0.2310 percent positivity rate for cannabinoid_receptor_antagonist\n",
            "0.1512 percent positivity rate for carbonic_anhydrase_inhibitor\n",
            "0.1512 percent positivity rate for casein_kinase_inhibitor\n",
            "0.0756 percent positivity rate for caspase_activator\n",
            "0.0504 percent positivity rate for catechol_o_methyltransferase_inhibitor\n",
            "0.4283 percent positivity rate for cc_chemokine_receptor_antagonist\n",
            "0.0756 percent positivity rate for cck_receptor_antagonist\n",
            "1.4277 percent positivity rate for cdk_inhibitor\n",
            "0.2268 percent positivity rate for chelating_agent\n",
            "0.1008 percent positivity rate for chk_inhibitor\n",
            "0.1764 percent positivity rate for chloride_channel_blocker\n",
            "0.2016 percent positivity rate for cholesterol_inhibitor\n",
            "0.2268 percent positivity rate for cholinergic_receptor_antagonist\n",
            "0.0252 percent positivity rate for coagulation_factor_inhibitor\n",
            "0.1512 percent positivity rate for corticosteroid_agonist\n",
            "1.8267 percent positivity rate for cyclooxygenase_inhibitor\n",
            "0.4367 percent positivity rate for cytochrome_p450_inhibitor\n",
            "0.1512 percent positivity rate for dihydrofolate_reductase_inhibitor\n",
            "0.1050 percent positivity rate for dipeptidyl_peptidase_inhibitor\n",
            "0.0252 percent positivity rate for diuretic\n",
            "0.2016 percent positivity rate for dna_alkylating_agent\n",
            "1.6881 percent positivity rate for dna_inhibitor\n",
            "0.5081 percent positivity rate for dopamine_receptor_agonist\n",
            "1.7805 percent positivity rate for dopamine_receptor_antagonist\n",
            "1.4109 percent positivity rate for egfr_inhibitor\n",
            "0.0252 percent positivity rate for elastase_inhibitor\n",
            "0.0042 percent positivity rate for erbb2_inhibitor\n",
            "0.6635 percent positivity rate for estrogen_receptor_agonist\n",
            "0.2016 percent positivity rate for estrogen_receptor_antagonist\n",
            "0.1512 percent positivity rate for faah_inhibitor\n",
            "0.0756 percent positivity rate for farnesyltransferase_inhibitor\n",
            "0.1050 percent positivity rate for fatty_acid_receptor_agonist\n",
            "0.2100 percent positivity rate for fgfr_inhibitor\n",
            "1.1716 percent positivity rate for flt3_inhibitor\n",
            "0.0756 percent positivity rate for focal_adhesion_kinase_inhibitor\n",
            "0.0756 percent positivity rate for free_radical_scavenger\n",
            "0.0966 percent positivity rate for fungal_squalene_epoxidase_inhibitor\n",
            "0.4451 percent positivity rate for gaba_receptor_agonist\n",
            "0.6929 percent positivity rate for gaba_receptor_antagonist\n",
            "0.2352 percent positivity rate for gamma_secretase_inhibitor\n",
            "1.1170 percent positivity rate for glucocorticoid_receptor_agonist\n",
            "0.0546 percent positivity rate for glutamate_inhibitor\n",
            "0.3107 percent positivity rate for glutamate_receptor_agonist\n",
            "1.5411 percent positivity rate for glutamate_receptor_antagonist\n",
            "0.0756 percent positivity rate for gonadotropin_receptor_agonist\n",
            "0.2520 percent positivity rate for gsk_inhibitor\n",
            "0.3023 percent positivity rate for hcv_inhibitor\n",
            "0.4451 percent positivity rate for hdac_inhibitor\n",
            "0.2478 percent positivity rate for histamine_receptor_agonist\n",
            "1.0120 percent positivity rate for histamine_receptor_antagonist\n",
            "0.1008 percent positivity rate for histone_lysine_demethylase_inhibitor\n",
            "0.1344 percent positivity rate for histone_lysine_methyltransferase_inhibitor\n",
            "0.3023 percent positivity rate for hiv_inhibitor\n",
            "1.1884 percent positivity rate for hmgcr_inhibitor\n",
            "0.3905 percent positivity rate for hsp_inhibitor\n",
            "0.1554 percent positivity rate for igf-1_inhibitor\n",
            "0.1260 percent positivity rate for ikk_inhibitor\n",
            "0.1302 percent positivity rate for imidazoline_receptor_agonist\n",
            "0.3065 percent positivity rate for immunosuppressant\n",
            "0.1260 percent positivity rate for insulin_secretagogue\n",
            "0.2142 percent positivity rate for insulin_sensitizer\n",
            "0.1764 percent positivity rate for integrin_inhibitor\n",
            "0.3863 percent positivity rate for jak_inhibitor\n",
            "1.1464 percent positivity rate for kit_inhibitor\n",
            "0.0252 percent positivity rate for laxative\n",
            "0.0252 percent positivity rate for leukotriene_inhibitor\n",
            "0.2604 percent positivity rate for leukotriene_receptor_antagonist\n",
            "0.0504 percent positivity rate for lipase_inhibitor\n",
            "0.2562 percent positivity rate for lipoxygenase_inhibitor\n",
            "0.0252 percent positivity rate for lxr_agonist\n",
            "0.1302 percent positivity rate for mdm_inhibitor\n",
            "0.3023 percent positivity rate for mek_inhibitor\n",
            "0.3107 percent positivity rate for membrane_integrity_inhibitor\n",
            "0.1050 percent positivity rate for mineralocorticoid_receptor_antagonist\n",
            "0.0504 percent positivity rate for monoacylglycerol_lipase_inhibitor\n",
            "0.3569 percent positivity rate for monoamine_oxidase_inhibitor\n",
            "0.0756 percent positivity rate for monopolar_spindle_1_kinase_inhibitor\n",
            "0.5459 percent positivity rate for mtor_inhibitor\n",
            "0.2016 percent positivity rate for mucolytic_agent\n",
            "0.1554 percent positivity rate for neuropeptide_receptor_antagonist\n",
            "3.4937 percent positivity rate for nfkb_inhibitor\n",
            "0.0252 percent positivity rate for nicotinic_receptor_agonist\n",
            "0.1092 percent positivity rate for nitric_oxide_donor\n",
            "0.0504 percent positivity rate for nitric_oxide_production_inhibitor\n",
            "0.1092 percent positivity rate for nitric_oxide_synthase_inhibitor\n",
            "0.0294 percent positivity rate for norepinephrine_reuptake_inhibitor\n",
            "0.0756 percent positivity rate for nrf2_activator\n",
            "0.2562 percent positivity rate for opioid_receptor_agonist\n",
            "0.4031 percent positivity rate for opioid_receptor_antagonist\n",
            "0.1554 percent positivity rate for orexin_receptor_antagonist\n",
            "0.2604 percent positivity rate for p38_mapk_inhibitor\n",
            "0.1008 percent positivity rate for p-glycoprotein_inhibitor\n",
            "0.2562 percent positivity rate for parp_inhibitor\n",
            "1.2472 percent positivity rate for pdgfr_inhibitor\n",
            "0.0756 percent positivity rate for pdk_inhibitor\n",
            "1.1086 percent positivity rate for phosphodiesterase_inhibitor\n",
            "0.1050 percent positivity rate for phospholipase_inhibitor\n",
            "0.6341 percent positivity rate for pi3k_inhibitor\n",
            "0.1302 percent positivity rate for pkc_inhibitor\n",
            "0.2310 percent positivity rate for potassium_channel_activator\n",
            "0.4115 percent positivity rate for potassium_channel_antagonist\n",
            "0.4829 percent positivity rate for ppar_receptor_agonist\n",
            "0.1260 percent positivity rate for ppar_receptor_antagonist\n",
            "0.4997 percent positivity rate for progesterone_receptor_agonist\n",
            "0.0756 percent positivity rate for progesterone_receptor_antagonist\n",
            "0.1512 percent positivity rate for prostaglandin_inhibitor\n",
            "0.3527 percent positivity rate for prostanoid_receptor_antagonist\n",
            "3.0486 percent positivity rate for proteasome_inhibitor\n",
            "0.2016 percent positivity rate for protein_kinase_inhibitor\n",
            "0.0252 percent positivity rate for protein_phosphatase_inhibitor\n",
            "0.4325 percent positivity rate for protein_synthesis_inhibitor\n",
            "0.0798 percent positivity rate for protein_tyrosine_kinase_inhibitor\n",
            "0.2352 percent positivity rate for radiopaque_medium\n",
            "0.9364 percent positivity rate for raf_inhibitor\n",
            "0.0504 percent positivity rate for ras_gtpase_inhibitor\n",
            "0.2813 percent positivity rate for retinoid_receptor_agonist\n",
            "0.0252 percent positivity rate for retinoid_receptor_antagonist\n",
            "0.1470 percent positivity rate for rho_associated_kinase_inhibitor\n",
            "0.1554 percent positivity rate for ribonucleoside_reductase_inhibitor\n",
            "0.1050 percent positivity rate for rna_polymerase_inhibitor\n",
            "0.9910 percent positivity rate for serotonin_receptor_agonist\n",
            "1.6965 percent positivity rate for serotonin_receptor_antagonist\n",
            "0.1848 percent positivity rate for serotonin_reuptake_inhibitor\n",
            "0.1512 percent positivity rate for sigma_receptor_agonist\n",
            "0.1512 percent positivity rate for sigma_receptor_antagonist\n",
            "0.1050 percent positivity rate for smoothened_receptor_antagonist\n",
            "1.1212 percent positivity rate for sodium_channel_inhibitor\n",
            "0.1050 percent positivity rate for sphingosine_receptor_agonist\n",
            "0.2981 percent positivity rate for src_inhibitor\n",
            "0.0252 percent positivity rate for steroid\n",
            "0.0798 percent positivity rate for syk_inhibitor\n",
            "0.2520 percent positivity rate for tachykinin_antagonist\n",
            "0.1260 percent positivity rate for tgf-beta_receptor_inhibitor\n",
            "0.0798 percent positivity rate for thrombin_inhibitor\n",
            "0.1554 percent positivity rate for thymidylate_synthase_inhibitor\n",
            "0.1260 percent positivity rate for tlr_agonist\n",
            "0.0294 percent positivity rate for tlr_antagonist\n",
            "0.1512 percent positivity rate for tnf_inhibitor\n",
            "0.5333 percent positivity rate for topoisomerase_inhibitor\n",
            "0.0756 percent positivity rate for transient_receptor_potential_channel_antagonist\n",
            "0.0252 percent positivity rate for tropomyosin_receptor_kinase_inhibitor\n",
            "0.1050 percent positivity rate for trpv_agonist\n",
            "0.2016 percent positivity rate for trpv_antagonist\n",
            "1.3270 percent positivity rate for tubulin_inhibitor\n",
            "0.3065 percent positivity rate for tyrosine_kinase_inhibitor\n",
            "0.0252 percent positivity rate for ubiquitin_specific_protease_inhibitor\n",
            "0.7139 percent positivity rate for vegfr_inhibitor\n",
            "0.1092 percent positivity rate for vitamin_b\n",
            "0.1638 percent positivity rate for vitamin_d_receptor_agonist\n",
            "0.1260 percent positivity rate for wnt_inhibitor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oc0du7bD7CT6"
      },
      "source": [
        "Two things:\n",
        "\n",
        "- Positivity rates are very low\n",
        "- Positivity rates are very heterogeneous"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csjXkhHB7KdF"
      },
      "source": [
        "Setting aside a validation set\n",
        "Let's set aside a training set and a validation set: all of our configuration choices will be guided by performance on this subset of the total available training data. We will also keep on the total available training data, which we will use to train our final production models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN4iVVq06-Ep",
        "outputId": "f0830a56-3da4-45cb-f4b1-f6f03f76258a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "num_train_samples = int(0.8 * len(train_features_df))\n",
        "\n",
        "full_train_features_ids = train_features_df.pop('sig_id')\n",
        "full_test_features_ids = test_features_df.pop('sig_id')\n",
        "train_targets_df.pop('sig_id')\n",
        "\n",
        "full_train_features_df = train_features_df.copy()\n",
        "full_train_targets_df = train_targets_df.copy()\n",
        "\n",
        "val_features_df = train_features_df[num_train_samples:]\n",
        "train_features_df = train_features_df[:num_train_samples]\n",
        "val_targets_df = train_targets_df[num_train_samples:]\n",
        "train_targets_df = train_targets_df[:num_train_samples]\n",
        "\n",
        "print('Total training samples:', len(full_train_features_df))\n",
        "print('Training split samples:', len(train_features_df))\n",
        "print('Validation split samples:', len(val_features_df))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total training samples: 23814\n",
            "Training split samples: 19051\n",
            "Validation split samples: 4763\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ys3F_MMC8AwR"
      },
      "source": [
        "<b>A dumb baseline</b><br>\n",
        "If you've read my book, you know you should start tough projects by computing a \"dumb\" baseline that will serve as your reference point. This is usually the highest score you can reach without looking at the test features (or validation features in this case). Let's use the positivity rate of each target as measured in the training subset to generate predictions for the validation subset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Jx1psM17u-B",
        "outputId": "fd67d983-eca2-485b-d229-65f812b80cdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predictions = []\n",
        "for target_name in list(train_targets_df):\n",
        "  rate = float(sum(train_targets_df[target_name])) / len(train_targets_df)\n",
        "  predictions.append(rate)\n",
        "\n",
        "predictions = np.array([predictions] * len(val_features_df))\n",
        "\n",
        "targets = np.array(val_targets_df)\n",
        "score = keras.losses.BinaryCrossentropy()(targets,predictions)\n",
        "print('Baseline score : %.4f' % score.numpy())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline score : 0.0209\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmHaXLzj80Tf"
      },
      "source": [
        "<b>Prepare TF datasets</b><br>\n",
        "Let's turn our dataframes into tf.data.Datasets, which we will use to train our Keras models in the next step. Our datasets will yield tuples of (features, targets) where features is a dict and targets is a list. In the features dict, we will have 3 keys: cp_type and cp_dose, as well as numerical_features, which will be a vector concatenating all numerical features in the space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9B4uRvkp8cUD"
      },
      "source": [
        "feature_names = list(train_features_df)\n",
        "categorical_feature_names = ['cp_type', 'cp_dose']\n",
        "numerical_feature_names = [name for name in feature_names if name not in categorical_feature_names]\n",
        "\n",
        "def merge_numerical_features(feature_dict):\n",
        "    categorical_features = {name: feature_dict[name] for name in categorical_feature_names}\n",
        "    numerical_features = tf.stack([tf.cast(feature_dict[name], 'float32') for name in numerical_feature_names])\n",
        "    feature_dict = categorical_features\n",
        "    feature_dict.update({'numerical_features': numerical_features})\n",
        "    return feature_dict"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLpDGXQj3eoq"
      },
      "source": [
        "tf.data : TensorFlow ìë ¥ íì´í ë¼ì¸ ë¹ë<br>\n",
        "https://www.tensorflow.org/guide/data?hl=ko"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3oXq14g9fJm"
      },
      "source": [
        "train_features_ds = tf.data.Dataset.from_tensor_slices(dict(train_features_df))\n",
        "train_features_ds = train_features_ds.map(lambda x: merge_numerical_features(x))\n",
        "\n",
        "train_targets_ds = tf.data.Dataset.from_tensor_slices(np.array(train_targets_df))\n",
        "train_ds = tf.data.Dataset.zip((train_features_ds, train_targets_ds))\n",
        "\n",
        "full_train_features_ds = tf.data.Dataset.from_tensor_slices(dict(full_train_features_df))\n",
        "full_train_features_ds = full_train_features_ds.map(lambda x: merge_numerical_features(x))\n",
        "full_train_targets_ds = tf.data.Dataset.from_tensor_slices(np.array(full_train_targets_df))\n",
        "full_train_ds = tf.data.Dataset.zip((full_train_features_ds, full_train_targets_ds))\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9E10CvkvzrRk",
        "outputId": "b234dd6c-4fb7-4834-90d1-7313bdb45821",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# dictííë¡ ì¤ë¹íë¤..\n",
        "train_features_ds"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<MapDataset shapes: {cp_type: (), cp_dose: (), numerical_features: (873,)}, types: {cp_type: tf.string, cp_dose: tf.string, numerical_features: tf.float32}>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcZZUpt_0OMc",
        "outputId": "1908a3c9-c708-4517-abee-91cde0eccd08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "full_train_features_ds"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<MapDataset shapes: {cp_type: (), cp_dose: (), numerical_features: (873,)}, types: {cp_type: tf.string, cp_dose: tf.string, numerical_features: tf.float32}>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfsXhMMt03Z4"
      },
      "source": [
        "val_features_ds = tf.data.Dataset.from_tensor_slices(dict(val_features_df))\n",
        "# dictííë¡ ë°ì´í°ë¥¼ ì¤ë¹íë¤ ì ê¸°..\n",
        "val_features_ds =val_features_ds.map(lambda x: merge_numerical_features(x))\n",
        "\n",
        "val_targets_ds = tf.data.Dataset.from_tensor_slices(np.array(val_targets_df))\n",
        "val_ds = tf.data.Dataset.zip((val_features_ds,val_targets_ds))\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices(dict(test_features_df))\n",
        "test_ds = test_ds.map(lambda x: merge_numerical_features(x))"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSesg8yP1qGG",
        "outputId": "67204ec5-10bc-4963-fe65-7a9d4d07cf46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# cardinalty() ì°¸ê³ ìë£ : https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
        "# ì§í©ì í¬ê¸°ë¥¼ íì\n",
        "print('Training split samples :', int(train_ds.cardinality()))\n",
        "print('Validation split samples:',int(val_ds.cardinality()))\n",
        "print('Test samples:',int(test_ds.cardinality()))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training split samples : 19051\n",
            "Validation split samples: 4763\n",
            "Test samples: 3982\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXpxpgLY2xxb"
      },
      "source": [
        "train_ds = train_ds.shuffle(1024).batch(64).prefetch(8)\n",
        "full_train_ds = full_train_ds.shuffle(1024).batch(64).prefetch(8)\n",
        "val_ds = val_ds.batch(64).prefetch(8)\n",
        "test_ds = test_ds.batch(64).prefetch(8)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzSh6mBO3BwA",
        "outputId": "1c40792e-0239-4e46-ddf4-966daa0e718b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "train_ds"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ({cp_type: (None,), cp_dose: (None,), numerical_features: (None, 873)}, (None, 206)), types: ({cp_type: tf.string, cp_dose: tf.string, numerical_features: tf.float32}, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqTI_cLr54F_"
      },
      "source": [
        "20ë 9ì 30ì¼ ìì ìë£ : íìíë¡ì° íì´íë¼ì¸ êµ¬ì¶"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3WATa2JknqH"
      },
      "source": [
        "<b>Encode our features</b><br>\n",
        "We use a StringLookup + CategoryEncoding layer to index and encode our string categorical features. It's a bit overkill since there are only two values, and it takes into account the possibility of unknown values at test time, which we don't have in this case. But it is very general and you can't go wrong with it.\n",
        "\n",
        "Then, we use a single Normalization layer to encode our concatenated numerical features.\n",
        "\n",
        "Finally, we concatenate the entire feature space into a single vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdYezLFi3Cfk"
      },
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
        "from tensorflow.keras.layers.experimental.preprocessing import CategoryEncoding\n",
        "from tensorflow.keras.layers.experimental.preprocessing import StringLookup"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdainrVhlK-X"
      },
      "source": [
        "def encode_numerical_feature(feature, name, dataset):\n",
        "    # Create a Normalization layer for our feature\n",
        "    normalizer = Normalization()\n",
        "\n",
        "    # Prepare a Dataset that only yields our feature\n",
        "    feature_ds = dataset.map(lambda x, y: x[name])\n",
        "\n",
        "    print(\"1. \", feature_ds)\n",
        "    # Learn the statistics of the data\n",
        "    normalizer.adapt(feature_ds)\n",
        "\n",
        "    # Normalize the input feature\n",
        "    encoded_feature = normalizer(feature)\n",
        "    return encoded_feature"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RpjNd46lvvz"
      },
      "source": [
        "def encode_categorical_feature(feature, name, dataset):\n",
        "    # Create a Lookup layer which will turn strings into integer indices\n",
        "    index = StringLookup()\n",
        "\n",
        "    # Prepare a Dataset that only yields our feature\n",
        "    feature_ds = dataset.map(lambda x, y: x[name])\n",
        "    print(\"2. \", feature_ds)\n",
        "    # Learn the set of possible feature values and assign them a fixed integer index\n",
        "    index.adapt(feature_ds)\n",
        "\n",
        "    # Turn the values into integer indices\n",
        "    encoded_feature = index(feature)\n",
        "    print(\"3. \", encoded_feature)\n",
        "    # Create a CategoryEncoding for our integer indices\n",
        "    encoder = CategoryEncoding(output_mode=\"binary\")\n",
        "\n",
        "    # Prepare a dataset of indices\n",
        "    feature_ds = feature_ds.map(index)\n",
        "    print(\"4. \", feature_ds)\n",
        "    # Learn the space of possible indices\n",
        "    encoder.adapt(feature_ds)\n",
        "    print(\"5. \", encoder)\n",
        "    # Apply one-hot encoding to our indices\n",
        "    encoded_feature = encoder(encoded_feature)\n",
        "    print(\"6. \", encoded_feature)\n",
        "    return encoded_feature"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POKWlP4Ql1Mi",
        "outputId": "f733b6e8-2801-4e7f-ce0d-28a28e495f55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "all_inputs = []\n",
        "all_encoded_features = []\n",
        "\n",
        "print('Processing categorical features...')\n",
        "for name in categorical_feature_names:\n",
        "    inputs = keras.Input(shape=(1,), name=name, dtype='string')\n",
        "    encoded = encode_categorical_feature(inputs, name, train_ds)\n",
        "    all_inputs.append(inputs)\n",
        "    all_encoded_features.append(encoded)\n",
        "\n",
        "print('Processing numerical features...')\n",
        "numerical_inputs = keras.Input(shape=(len(numerical_feature_names),), name='numerical_features')\n",
        "encoded_numerical_features = encode_numerical_feature(numerical_inputs, 'numerical_features', train_ds)\n",
        "\n",
        "all_inputs.append(numerical_inputs)\n",
        "all_encoded_features.append(encoded_numerical_features)\n",
        "features = layers.Concatenate()(all_encoded_features)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing categorical features...\n",
            "2.  <MapDataset shapes: (None,), types: tf.string>\n",
            "3.  Tensor(\"string_lookup_4/None_lookup_table_find/LookupTableFindV2:0\", shape=(None, 1), dtype=int64, device=/job:localhost/replica:0/task:0/device:CPU:0)\n",
            "4.  <MapDataset shapes: (None,), types: tf.int64>\n",
            "5.  <tensorflow.python.keras.layers.preprocessing.category_encoding.CategoryEncoding object at 0x7fd414076b00>\n",
            "6.  Tensor(\"category_encoding_4/bincount/DenseBincount:0\", shape=(None, 4), dtype=float32)\n",
            "2.  <MapDataset shapes: (None,), types: tf.string>\n",
            "3.  Tensor(\"string_lookup_5/None_lookup_table_find/LookupTableFindV2:0\", shape=(None, 1), dtype=int64, device=/job:localhost/replica:0/task:0/device:CPU:0)\n",
            "4.  <MapDataset shapes: (None,), types: tf.int64>\n",
            "5.  <tensorflow.python.keras.layers.preprocessing.category_encoding.CategoryEncoding object at 0x7fd4156d72e8>\n",
            "6.  Tensor(\"category_encoding_5/bincount/DenseBincount:0\", shape=(None, 4), dtype=float32)\n",
            "Processing numerical features...\n",
            "1.  <MapDataset shapes: (None, 873), types: tf.float32>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRFdMOgmn7c-"
      },
      "source": [
        "20ë 10ì 1ì¼ keras inputêµ¬ì¡° ì´í´ì¤"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Yz4XFjOoxou"
      },
      "source": [
        "<b>Train a basic model to establish a better baseline</b><br>\n",
        "Can a simple model beat our dumb baseline? Let's try a simple logistic regression over our concatenated feature space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MR0q07cLoxaa",
        "outputId": "63e3f15d-1eed-42bc-9fe5-54bf40e8a023",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 970
        }
      },
      "source": [
        "x = layers.Dropout(0.5)(features)\n",
        "outputs = layers.Dense(206, activation='sigmoid')(x)\n",
        "basic_model = keras.Model(all_inputs, outputs)\n",
        "basic_model.summary()\n",
        "basic_model.compile(optimizer=keras.optimizers.RMSprop(),\n",
        "                    loss=keras.losses.BinaryCrossentropy())\n",
        "basic_model.fit(full_train_ds, epochs=10, validation_data=val_ds)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "cp_type (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "cp_dose (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "string_lookup_4 (StringLookup)  (None, 1)            0           cp_type[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "string_lookup_5 (StringLookup)  (None, 1)            0           cp_dose[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "numerical_features (InputLayer) [(None, 873)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "category_encoding_4 (CategoryEn (None, 4)            1           string_lookup_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "category_encoding_5 (CategoryEn (None, 4)            1           string_lookup_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "normalization_2 (Normalization) (None, 873)          1747        numerical_features[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 881)          0           category_encoding_4[0][0]        \n",
            "                                                                 category_encoding_5[0][0]        \n",
            "                                                                 normalization_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 881)          0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 206)          181692      dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 183,441\n",
            "Trainable params: 181,692\n",
            "Non-trainable params: 1,749\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/10\n",
            "373/373 [==============================] - 27s 72ms/step - loss: 0.5939 - val_loss: 0.3009\n",
            "Epoch 2/10\n",
            "373/373 [==============================] - 27s 72ms/step - loss: 0.2529 - val_loss: 0.1161\n",
            "Epoch 3/10\n",
            "373/373 [==============================] - 27s 73ms/step - loss: 0.1364 - val_loss: 0.0587\n",
            "Epoch 4/10\n",
            "373/373 [==============================] - 27s 74ms/step - loss: 0.0895 - val_loss: 0.0277\n",
            "Epoch 5/10\n",
            "373/373 [==============================] - 27s 73ms/step - loss: 0.0672 - val_loss: 0.0200\n",
            "Epoch 6/10\n",
            "373/373 [==============================] - 28s 74ms/step - loss: 0.0516 - val_loss: 0.0175\n",
            "Epoch 7/10\n",
            "373/373 [==============================] - 28s 75ms/step - loss: 0.0415 - val_loss: 0.0162\n",
            "Epoch 8/10\n",
            "373/373 [==============================] - 28s 74ms/step - loss: 0.0345 - val_loss: 0.0158\n",
            "Epoch 9/10\n",
            "373/373 [==============================] - 28s 75ms/step - loss: 0.0299 - val_loss: 0.0157\n",
            "Epoch 10/10\n",
            "373/373 [==============================] - 27s 73ms/step - loss: 0.0267 - val_loss: 0.0157\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd41403f2b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udJsBkYumSc4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}